lda_pred <- predict(lda_model, newdata = test)
lda_pred
# Perform LDA on training set
lda_model <- lda(mpg01 ~ ., data = train)
lda_model
# Find test error (RMSE)
lda_pred <- predict(lda_model, newdata = test)
lda_rmse <- RMSE(lda_pred, test$mpg01)
lda_pred
test$mpg01
str(test)
class(lda_pred)
lda_pred$x
# Perform LDA on training set
lda_model <- lda(mpg01 ~ ., data = train)
lda_model
# Find test error (RMSE)
lda_pred <- predict(lda_model, newdata = test)
lda_rmse <- RMSE(lda_pred$x, test$mpg01)
print(paste("LDA RMSE:",lda_rmse))
# Perform LDA on training set
lda_model <- lda(mpg01 ~ ., data = train)
lda_model
# Find test error (RMSE)
lda_pred <- predict(lda_model, newdata = test)
lda_rmse <- RMSE(lda_pred$x, test$mpg01)
print(paste("LDA RMSE:",lda_rmse))
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Find test error (RMSE)
qda_pred <- predict(qda_model, newdata = test)
qda_rmse <- RMSE(qda_pred$x, test$mpg01)
print(paste("QDA RMSE:",qda_rmse))
qda_pred
# Find test error (RMSE)
qda_pred <- predict(qda_model, newdata = test, type = "response")
qda_pred
# Find test error (RMSE)
qda_pred <- predict(qda_model, newdata = test, type = "prob")
qda_pred
qda_pred$posterior
lda_pred
# Find test error (RMSE)
lda_pred <- predict(lda_model, newdata = test, type = "prob")
lda_pred
lda_pred$posterior[1]
lda_pred$posterior[1:]
lda_pred$posterior[1:nrow(lda_pred$posterior)]
# Classify predictions based on posterior probabilities
lda_pred_as_binary <- ifelse(lda_pred$posterior[1:nrow(lda_pred$posterior)] > 0.5, 0, 1)
lda_pred_as_binary
# Remove variables from local environment
rm(c(
lda_model,
lda_pred
))
# Remove variables from local environment
rm(
lda_model,
lda_pred
)
# Perform LDA on training set
lda_model <- lda(mpg01 ~ ., data = train)
lda_model
# Find test error (RMSE)
lda_pred <- predict(lda_model, newdata = test, type = "prob")
# Classify predictions based on posterior probabilities
lda_pred_as_binary <- ifelse(lda_pred$posterior[1:nrow(lda_pred$posterior)] > 0.5, 0, 1)
lda_rmse <- RMSE(lda_pred_as_binary, test$mpg01)
print(paste("LDA RMSE:",lda_rmse))
# Remove variables from local environment
rm(lda_model, lda_pred, lda_pred_as_binary, lda_rmse)
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Find test error (RMSE)
qda_pred <- predict(qda_model, newdata = test, type = "prob")
# Classify predictions based on posterior probabilities
qda_pred_as_binary <- ifelse(qda_pred$posterior[1:nrow(qda_pred$posterior)] > 0.5, 0, 1)
#qda_rmse <- RMSE(, test$mpg01)
print(paste("QDA RMSE:",qda_rmse))
# Remove variables from global environment
rm(qda_model, qda_pred, qda_pred_as_binary, qda_rmse)
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Find test error (RMSE)
qda_pred <- predict(qda_model, newdata = test, type = "prob")
# Classify predictions based on posterior probabilities
qda_pred_as_binary <- ifelse(qda_pred$posterior[1:nrow(qda_pred$posterior)] > 0.5, 0, 1)
#qda_rmse <- RMSE(, test$mpg01)
print(paste("QDA RMSE:",qda_rmse))
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Find test error (RMSE)
qda_pred <- predict(qda_model, newdata = test, type = "prob")
# Classify predictions based on posterior probabilities
qda_pred_as_binary <- ifelse(qda_pred$posterior[1:nrow(qda_pred$posterior)] > 0.5, 0, 1)
qda_rmse <- RMSE(qda_pred_as_binary, test$mpg01)
print(paste("QDA RMSE:",qda_rmse))
# Remove variables from global environment
#rm(qda_model, qda_pred, qda_pred_as_binary, qda_rmse)
qda_model %>% class()
# Perform LDA on training set
lda_model <- lda(mpg01 ~ ., data = train)
lda_model
lda_model %>% class()
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Find test error (RMSE)
qda_pred <- predict(qda_model, newdata = test, type = "prob")
# Classify predictions based on posterior probabilities
qda_pred_as_binary <- ifelse(qda_pred$posterior[1:nrow(qda_pred$posterior)] > 0.5, 0, 1)
qda_rmse <- RMSE(qda_pred_as_binary, test$mpg01)
print(paste("QDA RMSE:", qda_rmse))
# Remove variables from global environment
rm(qda_model, qda_pred, qda_pred_as_binary, qda_rmse)
logit_model <- glm(mpg01 ~ ., data = train, family = binomial)
logit_model
summary(logit_model)
# Perform logistic regression on training data
logit_model <- glm(mpg01 ~ ., data = train, family = binomial)
summary(logit_model)
# Generate predictions for RMSE calculation
logit_pred <- predict(logit_model, newdata = test, type = "prob")
# Generate predictions for RMSE calculation
logit_pred <- predict(logit_model, newdata = test, type = "response")
logit_pred
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Generate predictions for RMSE calculation
qda_pred <- predict(qda_model, newdata = test)
# Classify predictions based on posterior probabilities
qda_pred_as_binary <- ifelse(qda_pred$posterior[1:nrow(qda_pred$posterior)] > 0.5, 0, 1)
qda_rmse <- RMSE(qda_pred_as_binary, test$mpg01)
print(paste("QDA RMSE:", qda_rmse))
# Remove variables from global environment
rm(qda_model, qda_pred, qda_pred_as_binary, qda_rmse)
# Perform LDA on training set
lda_model <- lda(mpg01 ~ ., data = train)
lda_model
# Generate predictions for RMSE calculation
lda_pred <- predict(lda_model, newdata = test)
# Classify predictions based on posterior probabilities
lda_pred_as_binary <- ifelse(lda_pred$posterior[1:nrow(lda_pred$posterior)] > 0.5, 0, 1)
lda_rmse <- RMSE(lda_pred_as_binary, test$mpg01)
print(paste("LDA RMSE:",lda_rmse))
# Remove variables from global environment
rm(lda_model, lda_pred, lda_pred_as_binary, lda_rmse)
logit_pred
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Generate predictions for RMSE calculation
qda_pred <- predict(qda_model, newdata = test)
qda_Pred
qda_pred
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Generate predictions for RMSE calculation
qda_pred <- predict(qda_model, newdata = test)
# Classify predictions based on posterior probabilities
qda_pred_as_binary <- ifelse(qda_pred$posterior[1:nrow(qda_pred$posterior)] > 0.5, 0, 1)
qda_rmse <- RMSE(qda_pred_as_binary, test$mpg01)
print(paste("QDA RMSE:", qda_rmse))
# Remove variables from global environment
rm(qda_model, qda_pred, qda_pred_as_binary, qda_rmse)
# Perform logistic regression on training data
logit_model <- glm(mpg01 ~ ., data = train, family = binomial)
summary(logit_model)
# Generate predictions for RMSE calculation
logit_pred <- predict(logit_model, newdata = test, type = "response")
# Classify predictions based on posterior probabilities
logit_pred_as_binary <- ifelse(logit_pred > 0.5, 1, 0)
# Perform logistic regression on training data
logit_model <- glm(mpg01 ~ ., data = train, family = binomial)
summary(logit_model)
# Generate predictions for RMSE calculation
logit_pred <- predict(logit_model, newdata = test, type = "response")
# Classify predictions based on posterior probabilities
logit_pred_as_binary <- ifelse(logit_pred > 0.5, 1, 0)
logit_rmse <- RMSE(logit_pred_as_binary, test$mpg01)
print(paste("Logit RMSE:", logit_rmse))
# Perform LDA on training set
lda_model <- lda(mpg01 ~ ., data = train)
lda_model
# Generate predictions for RMSE calculation
lda_pred <- predict(lda_model, newdata = test)
# Classify predictions based on posterior probabilities
lda_pred_as_binary <- ifelse(lda_pred$posterior[1:nrow(lda_pred$posterior)] > 0.5, 0, 1)
lda_rmse <- RMSE(lda_pred_as_binary, test$mpg01)
print(paste("LDA Test Error (RMSE):",lda_rmse))
# Remove variables from global environment
rm(lda_model, lda_pred, lda_pred_as_binary, lda_rmse)
# Perform QDA on training set
qda_model <- qda(mpg01 ~ ., data = train)
qda_model
# Generate predictions for RMSE calculation
qda_pred <- predict(qda_model, newdata = test)
# Classify predictions based on posterior probabilities
qda_pred_as_binary <- ifelse(qda_pred$posterior[1:nrow(qda_pred$posterior)] > 0.5, 0, 1)
qda_rmse <- RMSE(qda_pred_as_binary, test$mpg01)
print(paste("QDA Test Error (RMSE):", qda_rmse))
# Remove variables from global environment
rm(qda_model, qda_pred, qda_pred_as_binary, qda_rmse)
# Perform logistic regression on training data
logit_model <- glm(mpg01 ~ ., data = train, family = binomial)
summary(logit_model)
# Generate predictions for RMSE calculation
logit_pred <- predict(logit_model, newdata = test, type = "response")
# Classify predictions based on posterior probabilities
logit_pred_as_binary <- ifelse(logit_pred > 0.5, 1, 0)
logit_rmse <- RMSE(logit_pred_as_binary, test$mpg01)
print(paste("Logit Test Error (RMSE):", logit_rmse))
# Perform logistic regression on training data
logit_model <- glm(mpg01 ~ ., data = train, family = binomial)
summary(logit_model)
# Generate predictions for RMSE calculation
logit_pred <- predict(logit_model, newdata = test, type = "response")
# Classify predictions based on posterior probabilities
logit_pred_as_binary <- ifelse(logit_pred > 0.5, 1, 0)
logit_rmse <- RMSE(logit_pred_as_binary, test$mpg01)
print(paste("Logit Test Error (RMSE):", logit_rmse))
# Remove variables from the global environment
rm(logit_model, logit_pred, logit_pred_as_binary, logit_rmse)
library(naivebayes)
library(e1071)
?naiveBayes
# Perform Naive Bayes on training data
bayes_model <- naiveBayes(mpg01 ~ ., data = train)
summary(bayes_model)
bayes_model
bayes_model
unique(df$mpg01)
tabulate(df$mpg01)
?tabulate
196/392
sum(df$mpg01)
sum(train$mpg01)
313-159
# Generate predictions for RMSE calculation
bayes_pred <- predict(bayes_model, newdata = test)
bayes_pred
# Calculate and Report Test Error as RMSE
logit_rmse <- RMSE(bayes_pred, test$mpg01)
print(paste("Naive Bayes Test Error (RMSE):", bayes_rmse))
# Calculate and Report Test Error as RMSE
logit_rmse <- RMSE(bayes_pred, test$mpg01)
bayes_pred
bayes_pred %>% class
library(e1071)
# Perform Naive Bayes on training data
bayes_model <- naiveBayes(mpg01 ~ ., data = train)
bayes_model
# Generate predictions for RMSE calculation
bayes_pred <- predict(bayes_model, newdata = test)
# Calculate and Report Test Error as RMSE
logit_rmse <- RMSE(as.numeric(bayes_pred), test$mpg01)
print(paste("Naive Bayes Test Error (RMSE):", bayes_rmse))
rm(logit_rmse)
bayes_pred %>% as.numeric()
# Generate predictions for RMSE calculation
bayes_pred <- predict(bayes_model, newdata = test) %>% as.numeric()
bayes_pred <- bayes_pred - 1
library(e1071)
# Perform Naive Bayes on training data
bayes_model <- naiveBayes(mpg01 ~ ., data = train)
bayes_model
# Generate predictions for RMSE calculation
bayes_pred <- predict(bayes_model, newdata = test) %>% as.numeric()
bayes_pred <- bayes_pred - 1
# Calculate and Report Test Error as RMSE
bayes_rmse <- RMSE(bayes_pred, test$mpg01)
print(paste("Naive Bayes Test Error (RMSE):", bayes_rmse))
sqrt(nrow(df))
sqrt(nrow(train))
?knn
knn_model
# Fit kNN model with k folds
knn_model <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10)
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
install.packages("caTools")
library(class)
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
library(class)
# K values to test
k_vals <- c(2,3,5,10,15,20,50)
# Iterate and fit kNN classifiers for each k
for (k in k_vals) {
# Fit kNN model with k folds
knn_model <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10)
knn_rmse <-
}
# Fit kNN model with k folds
knn_model <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10)
knn_model
# Fit kNN model with k folds
knn_model <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric
# Fit kNN model with k folds
knn_model <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric()
knn_model
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
library(class)
# K values to test
k_vals <- c(2,3,5,10,15,20,50)
# Iterate and fit kNN classifiers for each k
for (k in k_vals) {
# Fit kNN model with k folds and generate predictions
knn_pred <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric()
knn_pred <- knn_pred - 1
# Calculate test error (RMSE)
knn_rmse <- RMSE(knn_pred, test$mpg01)
print(paste("kNN Test Error (RMSE) for k =",k ,":", knn_rmse))
}
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
library(class)
# K values to test
k_vals <- c(2,3,5,10,sqrt(nrow(train)),20,50)
# Iterate and fit kNN classifiers for each k
for (k in k_vals) {
# Fit kNN model with k folds and generate predictions
knn_pred <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric()
knn_pred <- knn_pred - 1
# Calculate test error (RMSE)
knn_rmse <- RMSE(knn_pred, test$mpg01)
print(paste("kNN Test Error (RMSE) for k =",k ,":", knn_rmse))
}
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
library(class)
set.seed(17)
# K values to test
k_vals <- c(2,3,5,10,sqrt(nrow(train)),20,50)
# Iterate and fit kNN classifiers for each k
for (k in k_vals) {
# Fit kNN model with k folds and generate predictions
knn_pred <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric()
knn_pred <- knn_pred - 1
# Calculate test error (RMSE)
knn_rmse <- RMSE(knn_pred, test$mpg01)
print(paste("kNN Test Error (RMSE) for k =",k ,":", knn_rmse))
}
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
library(class)
set.seed(17)
# K values to test
k_vals <- c(2,3,5,10,sqrt(nrow(train)),20,50)
# Iterate and fit kNN classifiers for each k
for (k in k_vals) {
# Fit kNN model with k folds and generate predictions
knn_pred <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric()
knn_pred <- knn_pred - 1
# Calculate test error (RMSE)
knn_rmse <- RMSE(knn_pred, test$mpg01)
print(paste("kNN Test Error (RMSE) for k =",k ,":", knn_rmse))
}
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
library(class)
set.seed(17)
# K values to test
k_vals <- c(2,3,5,10,sqrt(nrow(train)),20,50)
# Iterate and fit kNN classifiers for each k
for (k in k_vals) {
# Fit kNN model with k folds and generate predictions
knn_pred <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric()
knn_pred <- knn_pred - 1
# Calculate test error (RMSE)
knn_rmse <- RMSE(knn_pred, test$mpg01)
print(paste("kNN Test Error (RMSE) for k =",k ,":", knn_rmse))
}
# Method for kNN found at https://www.geeksforgeeks.org/k-nn-classifier-in-r-programming/
library(caTools)
library(class)
set.seed(17)
# K values to test
k_vals <- c(2,3,5,10, sqrt(nrow(train)) ,20,50)
# Iterate and fit kNN classifiers for each k
for (k in k_vals) {
# Fit kNN model with k folds and generate predictions
knn_pred <- knn(train = train,
test = test,
cl = train$mpg01,
k = 10) %>% as.numeric()
knn_pred <- knn_pred - 1
# Calculate test error (RMSE)
knn_rmse <- RMSE(knn_pred, test$mpg01)
print(paste("kNN Test Error (RMSE) for k =",k ,":", knn_rmse))
}
0.8*80
?train
### File summary ###
# This file contains the construction of models that analyze the statistical difference among teams that
# are upset and teams that aren't over the course of 640 NCAA men's tournament games (2007-2019)
### Load data and libraries
library(tidyverse)
library(nnet)
library(hoopR)
library(janitor)
library(glmnet)
library(gam)
library(caret)
library(pROC)
library(car)
library(kableExtra)
# Contains regular season average stats for the winners and losers of all matchups in the NCAA tournament (2007-2019)
df <- read_csv("../../data/regular_season_stats_of_all_tournament_team_matchups_2007_2019.csv") %>%
drop_na()
setwd("~/ANLY512/5300-project-group-29/code/Models")
# Contains regular season average stats for the winners and losers of all matchups in the NCAA tournament (2007-2019)
df <- read_csv("../../data/regular_season_stats_of_all_tournament_team_matchups_2007_2019.csv") %>%
drop_na()
df <- df %>%
mutate(upset_char = ifelse(upset == 0, "expected", "upset"))
# Create variables with the difference in stats of winners and losers and the overall season stats
#df <- df %>%
#  mutate(victor_avg_points_season_plusminus = victor_avg_score - season_avg_score,
#         loser_avg_points_season_plusminus = loser_avg_score - season_avg_score,
#         victor_avg_opp_points_season_plusminus = victor_avg_opp_score - season_avg_opp_score,
#         loser_avg_opp_points_season_plusminus = loser_avg_opp_score - season_avg_opp_score)
# Train-test-validation split
set.seed(17)
train_val_idx <- sample(nrow(df), 0.8*nrow(df))
train_val <- df[train_val_idx,]
test <- df[-train_val_idx,]
val_idx <- sample(nrow(train_val), 0.2*nrow(train_val))
val <- train_val[val_idx,]
train <- train_val[-val_idx,]
rm(train_val, train_val_idx, val_idx)
# Non predictors
nonpreds <- c("seed_t1",
"seed_t2",
"round",
"season",
"region_name",
"name_t1",
"name_t2",
"victor",
"loser",
"score_t1",
"score_t2",
"victor_seed",
"loser_seed",
"region_number",
"upset")
preds <- colnames(train)[!colnames(train) %in% nonpreds]
preds <- preds[!startsWith(preds, "season")]
# Logistic model of mean regular season points scored compared to the average of all tournament teams
# Logistic regression specs
specs <- trainControl(method = "cv",
number = 10,
savePredictions = "all",
classProbs = TRUE)
# Train model with cross validation
model_logit <- train(upset_char ~ .,
data = train[preds],
method = "glm",
family = "binomial",
trControl = specs)
?train
preds
length(preds)
model_logit
summary(model_logit)
model_logit %>% kbl()
x <- summary(model_logit)
x$terms
x$aic
x$df
summary(model_logit) %>% data.frame()
data.frame(summary(model_logit))
data.frame(model_logit)
