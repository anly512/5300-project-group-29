
# Data Prep

```{r, show_col_types = FALSE}
library(readr)
library(tidyverse)

heat_check <- read_csv("../../data/heat_check_tournament_index.csv")
big_dance <- read_csv("../../data/Big_Dance_CSV.csv")
kenpom_data <- read_csv("../../data/kenpom_tournament_clean.csv")
tournament_matchups <- read_csv("../../data/tournament_matchups.csv")
upsetdata <- read_csv("../../data/regular_season_stats_of_all_tournament_team_matchups_2007_2019.csv")

# Inspect the data
head(heat_check)
head(big_dance)
head(kenpom_data)
head(tournament_matchups)
```
# Data Wrangling

```{r}
heat_check <- heat_check %>% rename(Season = YEAR)
big_dance <- big_dance %>% rename(Season = Year)
names(big_dance) <- str_replace(names(big_dance), "\\...\\d+$", "")
tournament_matchups <- tournament_matchups %>% rename(Season = YEAR)
kenpom_data <- kenpom_data %>% select(-1)
```

# Some EDA

```{r}
glimpse(heat_check)
summary(heat_check)
```

```{r}
# Histogram of PATH difficulty
Path_difficulty <- ggplot(heat_check, aes(x=PATH)) +
  geom_histogram(binwidth=1, fill="blue", color="white") +
  labs(title="Histogram of PATH Difficulty", x="PATH Difficulty", y="Count")
ggsave("../../plots/EDA/Histogram_of_PATH_Difficulty.png", plot = Path_difficulty, width = 4, height = 4, dpi = 300)
# Boxplot of POWER by SEED
Power_seed <- ggplot(heat_check, aes(x=factor(SEED), y=POWER)) +
  geom_boxplot() +
  labs(title="Boxplot of Team Power by Seed", x="Seed", y="Power")
ggsave("../../plots/EDA/Boxplot_of_Team_Power_by_Seed.png", plot = Power_seed, width = 4, height = 4, dpi = 300)
```

```{r}
# Correlation matrix of selected variables
selected_variables <- heat_check %>% select(POWER, PATH, WINS, `POOL VALUE`)
correlation_matrix <- cor(selected_variables, use="complete.obs")  # Handling missing values
print(correlation_matrix)

# Visualizing the correlation matrix
library(corrplot)
corrplot(correlation_matrix, method="circle")

# Save the correlation matrix plot
ggsave("../../plots/EDA/Correlation_Matrix_Plot.png", width = 4, height = 4, dpi = 300)


```
```{r}
# Scatter plot of POWER vs. PATH
power_path_scatter <- ggplot(heat_check, aes(x=POWER, y=PATH)) +
  geom_point(aes(color=factor(SEED)), alpha=0.6) +
  geom_smooth(method="lm") +
  labs(title="Scatter Plot of POWER vs. PATH by SEED", x="POWER", y="PATH")

print(power_path_scatter)

# Save the scatter plot
ggsave("../../plots/EDA/Scatter_Plot_of_POWER_vs_PATH.png", plot = power_path_scatter, width = 6, height = 4, dpi = 300)
```






```{r}

# Seed expectations map
seed_expectations <- c(
  `1`=5, `2`=4, `3`=4, `4`=3, `5`=3,
  `6`=2, `7`=2, `8`=2, `9`=1, `10`=1,
  `11`=1, `12`=1, `13`=0, `14`=0, `15`=0, `16`=0
)
# 1 means  elite eight or better
# 2 - 3 means sweet sixteen or better
# 4 -8 means round of 32 or better
# 9 - 12 means the team enter first round
# 13 - 16 means typically lose in first round


# Analyze upsets based on PATH difficulty
heat_check <- heat_check %>%
  mutate(Upset = ROUND > seed_expectations[as.character(SEED)],
         Upset = ifelse(Upset, "Upset", "No Upset"))

# Assigning the violin plot to a variable
violin_plot <- ggplot(heat_check, aes(x=Upset, y=PATH, fill=Upset)) +
  geom_violin(trim=FALSE) +
  labs(title="Distribution of Path Difficulty Based on Upset Status", x="Upset Status", y="Path Difficulty") +
  theme_minimal()

# Display the plot
print(violin_plot)

# Save the plot
ggsave("../../plots/EDA/Distribution_of_Path_Difficulty_Based_on_Upset_Status.png", plot = violin_plot, width = 6, height = 4, dpi = 300)


```

```{r}
# Load necessary libraries
library(dplyr)
library(e1071)  
library(caret)  
library(pROC)  # For AUC calculation

# Load and preprocess data
heat_check$Upset_numeric <- factor(heat_check$Upset, levels = c("No Upset", "Upset"))
heat_check$Upset_numeric <- as.integer(heat_check$Upset_numeric) - 1
heat_check <- heat_check %>%
  na.omit()  # Handling missing values

# Update feature selection
features <- heat_check %>% select(PATH, POWER, `POWER-PATH`, WINS, Upset_numeric)

# Seed for reproducibility and data partitioning
set.seed(74)
training_indices <- createDataPartition(features$Upset_numeric, p=0.8, list=FALSE)
training_data <- features[training_indices, ]
testing_data <- features[-training_indices, ]

# Train SVM model
svm_model <- svm(Upset_numeric ~ ., data=training_data, type='C-classification', kernel='radial')
summary(svm_model)

# Evaluate model on training data
train_predictions <- predict(svm_model, training_data)
train_conf_matrix <- confusionMatrix(as.factor(train_predictions), as.factor(training_data$Upset_numeric))
train_accuracy <- train_conf_matrix$overall['Accuracy']

# Evaluate model on testing data
test_predictions <- predict(svm_model, testing_data)
test_conf_matrix <- confusionMatrix(as.factor(test_predictions), as.factor(testing_data$Upset_numeric))
test_accuracy <- test_conf_matrix$overall['Accuracy']
test_precision <- test_conf_matrix$byClass['Precision']

# Calculate AUC for testing data
predictions_probs <- predict(svm_model, testing_data, type="prob")
roc_result <- roc(response = as.factor(testing_data$Upset_numeric), predictor = predictions_probs[,2])
auc_value <- auc(roc_result)
print(paste("AUC: ", auc_value))

# Print all results
print(paste("Training Accuracy: ", train_accuracy))
print(paste("Testing Accuracy: ", test_accuracy))
print(paste("Test Precision: ", test_precision))
print(paste("AUC: ", auc_value))

# Parameter tuning with cross-validation
train_control <- trainControl(method="cv", number=10, classProbs=TRUE, summaryFunction=twoClassSummary)
set.seed(123)
tuned_results := train(Upset_numeric ~ ., data=training_data, method="svmRadial",
                        trControl=train_control,
                        tuneLength=5,
                        metric="Accuracy")
validation_accuracy := ifelse(is.finite(max(tuned_results$results$Accuracy)), max(tuned_results$results$Accuracy), NA)
print(paste("Validation Accuracy: ", validation_accuracy))

```

```{r}
library(ggplot2)


svm_model_linear <- ksvm(Upset_numeric ~ ., data=training_data, kernel="vanilladot", type="C-svc")
predictions_linear <- predict(svm_model_linear, testing_data)
predictions_linear_factor <- factor(predictions_linear, levels = c("0", "1"))


actual_factor_linear <- factor(testing_data$Upset_numeric, levels = c("0", "1"))

# Create the confusion matrix
confusionMatrix_linear <- table(Predictions = predictions_linear_factor, Actual = actual_factor_linear)
print(confusionMatrix_linear)

# Calculate and print accuracy
accuracy_linear <- sum(diag(confusionMatrix_linear)) / sum(confusionMatrix_linear)
print(paste("Linear Model Accuracy: ", accuracy_linear))

# Visualization of the confusion matrix
conf_matrix_df <- as.data.frame(confusionMatrix_linear)
colnames(conf_matrix_df) <- c("Predictions", "Actual", "Freq")

cm_svm <- ggplot(data = conf_matrix_df, aes(x = Actual, y = Predictions, fill = Freq)) +
    geom_tile(color = "white") +  # Use tiles to create the matrix
    geom_text(aes(label = Freq), vjust = 1) +  # Add frequencies in the middle of tiles
    scale_fill_gradient(low = "lightblue", high = "firebrick") +  # Color gradient from blue to red
    labs(title = "Confusion Matrix Visualization", x = "Actual Labels", y = "Predicted Labels") +
    theme_minimal()  # Minimal theme for clean presentation
ggsave("../../plots/EDA/confusion_matrix_svm.png", plot = cm_svm, width = 4, height = 4, dpi = 300)
```



```{r}
library(PRROC)
pr_data <- pr.curve(scores.class0 = as.numeric(levels(predictions_linear_factor))[predictions_linear_factor], weights.class0 = as.numeric(actual_factor_linear) - 1, curve = TRUE)

# Convert the matrix to a data frame for ggplot
pr_df <- data.frame(
  recall = pr_data$curve[, "recall"],
  precision = pr_data$curve[, "precision"]
)

# Plot Precision-Recall Curve using ggplot
library(ggplot2)
ggplot(data = pr_df, aes(x = recall, y = precision)) +
  geom_line(color = "green") +
  geom_point(color = "darkgreen") +
  labs(title = "Precision-Recall Curve for SVM Model", x = "Recall", y = "Precision") +
  theme_minimal()

```

